# Module 4: Vision-Language-Action (VLA)

## Overview
This module focuses on the convergence of LLMs and Robotics. You'll learn about voice-to-action using OpenAI Whisper for voice commands, cognitive planning using LLMs to translate natural language ("Clean the room") into a sequence of ROS 2 actions, and complete a capstone project where a simulated robot receives a voice command, plans a path, navigates obstacles, identifies an object using computer vision, and manipulates it.

## Learning Objectives
- Implement voice-to-action systems using OpenAI Whisper
- Use LLMs for cognitive planning and natural language processing
- Translate natural language commands into ROS 2 action sequences
- Complete a comprehensive capstone project integrating all previous modules

## Lessons
1. [Voice-to-Action Whisper](./lesson-1-voice-to-action-whisper)
2. [Cognitive Planning LLMs](./lesson-2-cognitive-planning-llms)
3. [Integration Examples](./lesson-3-integration-examples)
4. [Capstone: Autonomous Humanoid](./lesson-4-capstone-autonomous-humanoid)

## Prerequisites
- Completion of Modules 1, 2, and 3
- Understanding of ROS 2, simulation, and AI integration
- Basic knowledge of natural language processing concepts

## Technology Stack
- OpenAI Whisper
- Large Language Models (LLMs)
- Computer Vision libraries
- Integration frameworks

## Next Steps
After completing this module, you'll have a comprehensive understanding of Physical AI & Humanoid Robotics and have completed the capstone project demonstrating these concepts in action.