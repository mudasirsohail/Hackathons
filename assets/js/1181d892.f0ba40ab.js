"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[3833],{7316:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>l,metadata:()=>o,toc:()=>r});const o=JSON.parse('{"id":"modules/module-4-vision-language-action/index","title":"Module 4: Vision-Language-Action (VLA)","description":"Overview","source":"@site/docs/modules/module-4-vision-language-action/index.md","sourceDirName":"modules/module-4-vision-language-action","slug":"/modules/module-4-vision-language-action/","permalink":"/Hackathons/docs/modules/module-4-vision-language-action/","draft":false,"unlisted":false,"editUrl":"https://github.com/mudasirsohail/Hackathons/docs/modules/module-4-vision-language-action/index.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Bipedal Locomotion","permalink":"/Hackathons/docs/modules/module-3-ai-robot-brain/lesson-4-bipedal-locomotion/"},"next":{"title":"Voice-to-Action Whisper","permalink":"/Hackathons/docs/modules/module-4-vision-language-action/lesson-1-voice-to-action-whisper/"}}');var s=i(4848),t=i(8453);const l={},a="Module 4: Vision-Language-Action (VLA)",c={},r=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Lessons",id:"lessons",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Technology Stack",id:"technology-stack",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:'This module focuses on the convergence of LLMs and Robotics. You\'ll learn about voice-to-action using OpenAI Whisper for voice commands, cognitive planning using LLMs to translate natural language ("Clean the room") into a sequence of ROS 2 actions, and complete a capstone project where a simulated robot receives a voice command, plans a path, navigates obstacles, identifies an object using computer vision, and manipulates it.'}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement voice-to-action systems using OpenAI Whisper"}),"\n",(0,s.jsx)(n.li,{children:"Use LLMs for cognitive planning and natural language processing"}),"\n",(0,s.jsx)(n.li,{children:"Translate natural language commands into ROS 2 action sequences"}),"\n",(0,s.jsx)(n.li,{children:"Complete a comprehensive capstone project integrating all previous modules"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"lessons",children:"Lessons"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./lesson-1-voice-to-action-whisper",children:"Voice-to-Action Whisper"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./lesson-2-cognitive-planning-llms",children:"Cognitive Planning LLMs"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./lesson-3-integration-examples",children:"Integration Examples"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./lesson-4-capstone-autonomous-humanoid",children:"Capstone: Autonomous Humanoid"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Completion of Modules 1, 2, and 3"}),"\n",(0,s.jsx)(n.li,{children:"Understanding of ROS 2, simulation, and AI integration"}),"\n",(0,s.jsx)(n.li,{children:"Basic knowledge of natural language processing concepts"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"technology-stack",children:"Technology Stack"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"OpenAI Whisper"}),"\n",(0,s.jsx)(n.li,{children:"Large Language Models (LLMs)"}),"\n",(0,s.jsx)(n.li,{children:"Computer Vision libraries"}),"\n",(0,s.jsx)(n.li,{children:"Integration frameworks"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"After completing this module, you'll have a comprehensive understanding of Physical AI & Humanoid Robotics and have completed the capstone project demonstrating these concepts in action."})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>a});var o=i(6540);const s={},t=o.createContext(s);function l(e){const n=o.useContext(t);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),o.createElement(t.Provider,{value:n},e.children)}}}]);